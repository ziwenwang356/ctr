一，bootstraping:自助法，有放回抽样。非参数统计中一种重要的估计统计量方差进而进行区间估计的统计方法；

1，采用重抽样技术从原始样本中抽取一定数量的样本；

2，根据抽出的样本计算给定的统计量T；

3，重复上述N次（一般大于1000），得到N个统计量T；

4，计算上述N个统计量T的样本方差，得到统计量的方差；

bootstrap是现代统计学较为流行的一种统计方法，在小样本时效果很好，通过方差的估计可以构造置信区间等，其运用范围得到进一步延伸。


二，bagging:bootstrap aggregating的缩写。

1，让该学习算法训练多轮，每轮的训练集由从初始的训练集中随机取出的n个训练样本组成；

2，某个初始训练样本在某轮训练集中可以出现多次或者根本上不出现，训练之后可得到一个预测函数序列h_1,…,h_n,最终的预测函数H对分类问题采用投票方式，对
回归问题采用简单平均方法对新示例进行判别；

训练R个分类器f_i,分类器除了参数不同外，其他都是相同的。其中f_i是通过从训练集合中(N篇文档)随机取（有放回）N次文档构成的训练集合训练得到的。对于新文档d,
用这R个分类器去分类，得到的最多的是那个类别作为d的最终类别；



三，boosting,其中主要的是AdaBoost(adaptive boosting)

1,初始化时对每一个训练例赋相等的权重1/n,然后用该算法对训练集训练t轮，每次训练后，对训练失败的例子赋予较大的权重，也就是让学习算法在后续的学习中集中对比较难的训练例
进行学习，从而得到一个预测序列 h_1,…,h_m,其中 h_i也有一定的权重，预测效果好的预测函数权重较大，反之较小。

2，最终的预测函数H对分类问题采用有权重的投票方式，对回归问题采用加权平均的方法，对新示例进行判别；

类似Bagging方法，但是训练是串行进行的，第k个分类器训练时关注对前k-1分类器中错分的文档，即不是随机取，而是加大取这些文档的概率；


四，Bagging 与Boosting的区别：

1，Bagging采用均匀取样，而Boosting根据错误率来取样，因此后者的分类精度要优于前者；

2，Bagging的训练集的选择是随机的，各个训练集之间相互独立，而Boosting的各轮训练集的选择与前面各轮的学习结果有关；

3，Bagging的各个预测函数没有权重，而Boosting是有权重的；

4，Bagging的各个预测函数可以并行生成，而Boosting的各个预测函数值能顺序生成。

5，对于像神经网络这样极为耗时的学习方法，bagging可通过并行训练节省大量时间开销；

6，bagging和boosting都可以有效的提高分类的准确性，在大多数数据集中，boosting的准确性比bagging高，在有些数据集中，boosting会引起退化。boosting思想的一种改进版本 AdaBoosting方法在邮件过滤，文本分类方面都有很好的性能。


五，gradient boosting ,是一种实现boosting的方法。

1，主要思想是：每一次建立模型是在之前建立模型损失函数的梯度下降方向。

2，损失函数 loss function描述的是模型的不靠谱程度，损失函数越大，则说明模型容易出错。如果我们的模型能够让损失函数持续的下降，则说明我们的模型在不停的改进，而最好的方式就是让损失函数在其梯度 gradient方向上 下降。


六，Rand forest,随机森林

1，是用随机的方式建立一个森林，森林是由许多的决策树组成。随机森林的每一棵决策树之间没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类，然后看看哪一类选择最多，就预测这个样本为那一类。

2，在建立每一棵决策树的过程中，有两点需要注意，采样与完全分裂。首先是两个随机采样的过程，random forest对输入的数据要进行行，列的采样。对于行采样，采用有放回的方式，这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting,然后进行列采样，从M个feature中，选择m个（m<<M）.之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂，要么是里面的所有眼版本都是指向同一个分类。

3，一般很多的决策树算法都有一个重要的步骤，剪枝。但是这里不这么做。由于之前的两个随机采样的过程保证了随机性。所以就算不剪枝，也不会出现over-fitting.按这种算法得到的随机森林中的每一棵都是很弱的。但是大家组合起来就很厉害了。可以这样比喻随机森林算法：每一棵决策树就是一个精通于某一个狭窄的专家，这样在随机森林中就有了很多个精通不同领域的专家，对一个新问题，可以用不同的角度去对待它，最终由各个专家，投票得到结果。


